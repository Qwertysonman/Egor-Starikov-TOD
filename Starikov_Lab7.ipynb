{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tC2zp9oU-Enn"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import Levenshtein\n",
        "\n",
        "# with open('litw-win.txt', 'r', encoding='cp1251') as file:\n",
        "#     words = file.read().split()\n",
        "#\n",
        "# import Levenshtein\n",
        "# sentence='''с велечайшим усилием выбравшись из потока убегающих людей Кутузов со свитой уменьшевшейся вдвое поехал на звуки выстрелов русских орудий'''\n",
        "# for i, word in enumerate(sentence.split()):\n",
        "#     if word not in words:\n",
        "#         min_distance = float('inf')\n",
        "#         nearest_word = ''\n",
        "#         for word_in_dict in words:\n",
        "#             distance = Levenshtein.distance(word, word_in_dict)\n",
        "#             if distance < min_distance:\n",
        "#                 min_distance = distance\n",
        "#                 nearest_word = word_in_dict\n",
        "#         sentence = sentence.replace(word, nearest_word, 1)\n",
        "# print(sentence)\n",
        "# 2\n",
        "text = \"Считайте слова из файла litw-win.txt и запишите их в список words. В заданном предложении исправьте все опечатки, заменив слова с опечатками на ближайшие (в смысле расстояния Левенштейна) к ним слова из списка words. Считайте, что в слове есть опечатка, если данное слово не содержится в списке words. Разбейте этот текст из формулировки на слова; проведите стемминг и лемматизацию слов.\"\n",
        "#\n",
        "import re\n",
        "words = re.findall(r'\\w+', text)\n",
        "# print(words)\n",
        "from nltk.stem import SnowballStemmer\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "russian = SnowballStemmer('russian')\n",
        "\n",
        "stemmed_words = [russian.stem(word) for word in words]\n",
        "\n",
        "# print(stemmed_words)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = []\n",
        "for word in words:\n",
        "    pos = wordnet.VERB\n",
        "    if wordnet.synsets(word):\n",
        "        pos = wordnet.synsets(word)[0].pos()\n",
        "\n",
        "    lemmatized_word = lemmatizer.lemmatize(word, pos)\n",
        "    lemmatized_words.append(lemmatized_word)\n",
        "\n",
        "# print(lemmatized_words)\n",
        "\n",
        "# 3\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "#\n",
        "# sentences =words\n",
        "#\n",
        "# vectorizer = CountVectorizer()\n",
        "# X = vectorizer.fit_transform(sentences)\n",
        "#\n",
        "#\n",
        "# print(X.toarray())\n",
        "# print(vectorizer.get_feature_names_out())\n",
        "# # 1\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "df = pd.read_csv('preprocessed_descriptions.csv')\n",
        "sentences = df['preprocessed_descriptions'].tolist()\n",
        "\n",
        "words = set()\n",
        "for sentence in sentences:\n",
        "    try:\n",
        "        words.update(word_tokenize(sentence))\n",
        "    except TypeError:\n",
        "        pass\n",
        "\n",
        "# print(words)\n",
        "# 1.2\n",
        "# import random\n",
        "# import editdistance\n",
        "#\n",
        "# df = pd.read_csv('preprocessed_descriptions.csv')\n",
        "# sentences = df['preprocessed_descriptions'].tolist()\n",
        "# words = set()\n",
        "# for sentence in sentences:\n",
        "#     try:\n",
        "#         words.update(word_tokenize(sentence))\n",
        "#     except TypeError:\n",
        "#         pass\n",
        "#\n",
        "# word_pairs = random.sample(list(words), k=10)\n",
        "# word_pairs = [(word_pairs[i], word_pairs[i+1]) for i in range(0, len(word_pairs), 2)]\n",
        "# for pair in word_pairs:\n",
        "#     distance = editdistance.eval(pair[0], pair[1])\n",
        "#     print(f\"Расстояние между '{pair[0]}' и '{pair[1]}': {distance}\")\n",
        "# 1.3\n",
        "# import Levenshtein\n",
        "#\n",
        "# def find_closest_words(word, words, k):\n",
        "#     distances = [(w, Levenshtein.distance(word, w)) for w in words]\n",
        "#     distances.sort(key=lambda x: x[1])\n",
        "#     return [w[0] for w in distances[:k]]\n",
        "#\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#\n",
        "\n",
        "#\n",
        "\n",
        "#\n",
        "# vectorizer = TfidfVectorizer()\n",
        "#\n",
        "# for index, row in random_recipes.iterrows():\n",
        "#     description = row['preprocessed_descriptions']\n",
        "#     vector = vectorizer.fit_transform([description])\n",
        "#     print(f\"Рецепт {index}: {vector.toarray()}\")\n",
        "\n",
        "# from scipy.spatial.distance import cosine\n",
        "# import numpy as np\n",
        "# recipes = pd.read_csv('preprocessed_descriptions.csv')\n",
        "# random_recipes = recipes.sample(n=5)\n",
        "# n = len(random_recipes)\n",
        "# similarity_matrix = np.zeros((n, n))\n",
        "#\n",
        "# for i in range(n):\n",
        "#     for j in range(n):\n",
        "#         similarity_matrix[i][j] = 1 - cosine(random_recipes.iloc[i]['preprocessed_descriptions'], random_recipes.iloc[j]['preprocessed_descriptions'])\n",
        "#\n",
        "# similarity_df = pd.DataFrame(similarity_matrix, columns=random_recipes.index, index=random_recipes.index)\n",
        "#\n",
        "# print(similarity_df)"
      ]
    }
  ]
}